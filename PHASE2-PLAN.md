# Phase 2: AI Optimization - Implementation Plan

**Created**: November 12, 2025
**Status**: In Planning
**Duration**: Months 3-4 (8 weeks)
**Research Date**: November 12, 2025

---

## Executive Summary

Phase 2 transforms the SEO/GEO Optimizer from **analysis-only** to **implementation-capable**. Based on 2025 research analyzing 41M AI search results and 680M citations, this phase adds content rewriting, platform-specific optimization, and automated enhancement capabilities.

**Key Insight**: AI-referred traffic grew 527% (Jan-May 2025). Content with proper structure receives 40% more citations. Statistics Addition and Quotation Addition show 41% and 28% improvements over baseline.

---

## Research Foundation (2025 Data)

### GEO Best Practices

**Content Structure** (Princeton/Georgia Tech research):
- Short paragraphs: 60-100 words each
- Short sentences: 15-20 words maximum
- FAQ clusters: 40-60 word answers
- Evidence tables: Extractable facts with citations
- H2â†’H3â†’bullet structure: 40% more citations

**Proven Optimization Methods**:
- Statistics Addition: +41% improvement
- Quotation Addition: +28% improvement  
- User-generated content: 450% citation surge (Reddit now 7.15%)
- Freshness: Content updated within 30 days = 3.2x more citations

### Platform-Specific Citation Patterns

**ChatGPT** (40-60% of LLM traffic):
- Favors: Wikipedia (1.3M citations), G2 (196K), Forbes (181K)
- Structure: Answer-first, listicles, how-to guides (+35% citations)
- Schema: FAQPage and Article essential
- Credentials: Author credentials = +40% citation boost

**Perplexity** (Specialized/fresh content):
- Critical factor: Freshness (update every 2-3 days ideal, minimum 90 days)
- Structure: H2â†’H3â†’bullets (40% more citations)
- First hours: Critical for initial traction
- Prefers: Comprehensive guides, original research, comparisons

**Claude** (Growing, accuracy-focused):
- Attribution: 91.2% correct source attribution
- Citations: 5-8 primary sources with publisher + year
- Format: Inline citations, clickable links
- Recency: Current content for time-sensitive queries

**Gemini** (Google ecosystem):
- Traditional authority signals
- Community validation (Reddit-style)
- Google My Business integration
- Local Pack optimization

### Voice Search (2025)

**Market Size**:
- 153.5M US users (20.5% globally)
- 75% of US households own smart speaker
- Featured snippets: 40.7% of voice answers

**Optimization**:
- Answer length: 30-40 words for featured snippets
- Speakable schema: 20-30 seconds (2-3 sentences) per section
- FAQ-style content with natural language
- Long-tail, question-based keywords

---

## Phase 2 Architecture

### What We Have (Phase 1)

âœ… **Analysis Layer**:
- `analyze_content.py` - Extract metadata, structure, content
- `metadata_validator.py` - Validate meta tags, OG, Twitter
- `keyword_analyzer.py` - Extract keywords (primary, LSI, long-tail)
- `entity_extractor.py` - Extract persons, orgs, places
- `schema_generator.py` - Generate JSON-LD schemas
- `audit_report.py` - Generate reports (JSON, MD, HTML)

### What We're Adding (Phase 2)

ðŸ†• **Implementation Layer**:
- `content_optimizer.py` - Rewrite content for AI citation
- `platform_optimizer.py` - Apply platform-specific enhancements
- `voice_optimizer.py` - Enhance for voice search
- `freshness_monitor.py` - Track content age, recommend updates
- `citation_enhancer.py` - Add statistics, quotations, evidence tables
- `auto_implementer.py` - Orchestrate all optimizations

---

## Implementation Plan

### Week 1-2: Content Rewriting Engine

#### Script: `content_optimizer.py`

**Purpose**: Rewrite existing content following GEO best practices.

**Core Functions**:

```python
def optimize_paragraph_length(text: str) -> str:
    """Break long paragraphs into 60-100 word chunks"""
    # Split paragraphs >100 words
    # Maintain semantic coherence
    # Return restructured text

def optimize_sentence_length(text: str) -> str:
    """Split long sentences into 15-20 word segments"""
    # Detect sentences >20 words
    # Split at natural break points (commas, conjunctions)
    # Preserve meaning

def add_tldr_section(content: Dict) -> Dict:
    """Generate TL;DR in first 40-60 words"""
    # Extract main topic from H1
    # Synthesize key points from H2s
    # Create direct answer (40-60 words)
    # Insert at content start

def create_faq_section(content: Dict) -> List[Dict]:
    """Generate FAQ from H2/H3 headings"""
    # Convert headings to questions
    # Extract/synthesize 40-60 word answers
    # Natural language format
    # Return FAQ schema + HTML

def add_evidence_tables(content: Dict) -> str:
    """Create extractable fact tables"""
    # Identify statistics in text
    # Extract claims with numbers
    # Format as table: Metric | Value | Source
    # Add schema markup

def enhance_heading_structure(content: Dict) -> Dict:
    """Optimize H2â†’H3â†’bullet hierarchy"""
    # Ensure single H1
    # Convert long paragraphs after H2 to H3+bullets
    # Add question-based H2s for voice search
    # Return restructured HTML
```

**Input**: HTML/Markdown file path
**Output**: Optimized HTML/Markdown file
**Mode**: Create new file or replace original (user choice)

**Quality Gates**:
- Preserve all facts and claims (no hallucination)
- Maintain original meaning and intent
- All statistics must have existing sources
- New headings must reflect existing content

---

### Week 2-3: Platform-Specific Optimization

#### Script: `platform_optimizer.py`

**Purpose**: Apply platform-specific enhancements based on target AI engines.

**Core Functions**:

```python
def optimize_for_chatgpt(content: Dict) -> Dict:
    """ChatGPT-specific optimization"""
    # Add author credentials prominently
    # Ensure 1500-2500 word count
    # Add citations to primary sources (PubMed, arXiv)
    # Generate Article schema with E-E-A-T
    # Format: Answer-first, listicles
    # Return enhanced content

def optimize_for_perplexity(content: Dict) -> Dict:
    """Perplexity-specific optimization"""
    # Update dateModified to current date
    # Add inline citations with [1], [2] format
    # Ensure H2â†’H3â†’bullet structure
    # Add "Last Updated" prominent display
    # Include current statistics (2024-2025)
    # Return enhanced content

def optimize_for_claude(content: Dict) -> Dict:
    """Claude-specific optimization"""
    # Add 5-8 primary source citations (publisher + year)
    # Transparent methodology section
    # Acknowledge limitations explicitly
    # Add "Data Sources" section
    # Inline citations with clickable links
    # Return enhanced content

def optimize_for_gemini(content: Dict) -> Dict:
    """Gemini-specific optimization"""
    # Add user reviews/testimonials
    # Generate Organization schema with GMB
    # Add local citations (NAP consistency)
    # Include community validation signals
    # Traditional authority signals (awards, press)
    # Return enhanced content

def optimize_multi_platform(content: Dict, platforms: List[str]) -> Dict:
    """Apply optimizations for multiple platforms"""
    # Combine strategies for selected platforms
    # Resolve conflicts (e.g., length preferences)
    # Prioritize universal best practices
    # Return balanced optimization
```

**Conflict Resolution**:
- ChatGPT wants 1500-2500 words, Perplexity wants freshness
- Solution: Comprehensive content + weekly updates
- Claude wants primary sources, ChatGPT wants Wikipedia
- Solution: Mix both (Wikipedia as intro, primary for depth)

---

### Week 3-4: Voice Search Enhancement

#### Script: `voice_optimizer.py`

**Purpose**: Enhance content for voice assistants (Google Assistant, Siri, Alexa).

**Core Functions**:

```python
def generate_featured_snippet_content(content: Dict) -> str:
    """Create 30-40 word summary for featured snippets"""
    # Extract main question/topic
    # Synthesize direct answer (30-40 words)
    # Use simple language (8th grade level)
    # Return snippet-ready text

def add_speakable_schema(content: Dict) -> Dict:
    """Add speakable schema to key sections"""
    # Identify 2-3 sentence segments (20-30 seconds)
    # Natural language, conversational tone
    # Add Speakable schema markup
    # Return content with schema

def optimize_for_questions(content: Dict) -> Dict:
    """Add question-based content"""
    # Generate who/what/where/when/why/how questions
    # Create natural language answers
    # Format as FAQ with short answers
    # Add FAQPage schema
    # Return enhanced content

def optimize_readability_for_voice(text: str) -> str:
    """Simplify language for TTS"""
    # Replace jargon with plain language
    # Simplify complex sentences
    # Remove parenthetical asides
    # Add pronunciation hints for technical terms
    # Return voice-friendly text

def generate_howto_schema(content: Dict) -> Dict:
    """Create HowTo schema for voice search"""
    # Extract step-by-step instructions
    # Format with durations (ISO 8601)
    # Add images for each step
    # Generate HowTo JSON-LD
    # Return schema
```

**Voice Optimization Targets**:
- Featured snippets: 40.7% of voice answers
- Answer length: 30-40 words ideal
- Reading level: 8th grade or lower
- Schema: Speakable + FAQ + HowTo

---

### Week 4-5: Freshness Monitoring

#### Script: `freshness_monitor.py`

**Purpose**: Track content age and recommend updates based on platform requirements.

**Core Functions**:

```python
def analyze_content_freshness(file_path: str) -> Dict:
    """Check content freshness indicators"""
    # Extract datePublished, dateModified from schema
    # Calculate days since last update
    # Identify dated content (years, "2023", "last year")
    # Check statistics currency (years in data)
    # Return freshness report

def recommend_update_frequency(content: Dict, platform: str) -> Dict:
    """Platform-specific update recommendations"""
    # Perplexity: Every 2-3 days (aggressive) or 90 days (minimum)
    # ChatGPT: Quarterly for evergreen, monthly for trends
    # Claude: When new research available (primary sources)
    # Gemini: Monthly for local, quarterly for general
    # Return schedule

def identify_outdated_elements(content: Dict) -> List[Dict]:
    """Find specific outdated content"""
    # Statistics with years (e.g., "2023: 10M users")
    # Dated language ("last year", "recently" >6 months old)
    # Old screenshots, images with dates
    # Deprecated practices or tools
    # Return list of issues with locations

def generate_update_recommendations(content: Dict) -> List[str]:
    """Specific actions to refresh content"""
    # Replace old statistics with current (source suggestions)
    # Update dateModified to today
    # Add "Last Updated: [Date]" prominent display
    # Refresh screenshots if applicable
    # Add recent case studies or examples
    # Return actionable list

def schedule_auto_updates(file_paths: List[str], config: Dict) -> Dict:
    """Create update schedule for multiple files"""
    # Analyze each file's platform targets
    # Determine optimal update frequency
    # Generate calendar schedule
    # Create monitoring alerts
    # Return schedule configuration
```

**Monitoring Intervals**:
- High priority (Perplexity focus): Weekly check, update every 2-3 days
- Medium priority (Multi-platform): Monthly check, update quarterly
- Low priority (Evergreen): Quarterly check, update annually

**Automation**:
- Cron job integration (optional)
- Email alerts when updates due
- Batch update recommendations
- Priority ranking (critical â†’ low)

---

### Week 5-6: Citation Enhancement

#### Script: `citation_enhancer.py`

**Purpose**: Add statistics, quotations, and evidence tables (41% and 28% improvement).

**Core Functions**:

```python
def identify_citation_opportunities(content: Dict) -> List[Dict]:
    """Find locations to add statistics/quotations"""
    # Claims without numbers
    # General statements that could be quantified
    # Sections without expert quotes
    # Evidence-heavy topics (medical, technical)
    # Return opportunities with context

def suggest_statistics(context: str, topic: str) -> List[Dict]:
    """Suggest relevant statistics to add"""
    # Analyze paragraph context
    # Identify quantifiable claims
    # Suggest statistic types (percentages, growth, comparison)
    # Provide example format
    # Return suggestions with placeholders
    
    # NOTE: Does NOT fetch actual statistics (user must provide)
    # Suggests WHERE and WHAT TYPE of statistics to add

def suggest_quotations(context: str, topic: str) -> List[Dict]:
    """Suggest where to add expert quotations"""
    # Identify opinion statements
    # Find sections needing authority
    # Suggest quotation sources (researchers, practitioners)
    # Provide quotation format
    # Return suggestions with placeholders
    
    # NOTE: Does NOT generate fake quotes
    # Suggests WHERE quotes would strengthen content

def create_evidence_table(facts: List[Dict]) -> str:
    """Format facts as extractable evidence table"""
    # Structure: Metric | Value | Source | Year
    # Markdown table format
    # Add Table schema markup
    # Return table HTML/Markdown

def add_inline_citations(text: str, citations: List[Dict]) -> str:
    """Add citation markers to text"""
    # Format: [1], [2] or (Source, Year)
    # Link to references section
    # Clickable citations (Claude preference)
    # Return text with citations

def generate_references_section(citations: List[Dict]) -> str:
    """Create formatted references section"""
    # Sort by citation order
    # Format: [1] Author. Title. Publisher, Year. URL
    # Add schema markup (CitationEntity)
    # Return HTML/Markdown section
```

**Quality Standards**:
- NO fabricated statistics (user must provide real data)
- NO generated quotations (suggest existing quotes)
- All citations must have verifiable sources
- Evidence tables only from existing content facts

---

### Week 6-7: Auto-Implementer Orchestration

#### Script: `auto_implementer.py`

**Purpose**: Orchestrate all Phase 2 optimizations in correct sequence.

**Core Functions**:

```python
def auto_optimize_content(file_path: str, config: Dict) -> Dict:
    """Full automated optimization pipeline"""
    # 1. Run Phase 1 analysis (audit_report.py)
    # 2. Apply content optimization (content_optimizer.py)
    # 3. Apply platform optimization (platform_optimizer.py)
    # 4. Apply voice optimization (voice_optimizer.py)
    # 5. Add citation enhancements (citation_enhancer.py)
    # 6. Validate output (metadata_validator.py)
    # 7. Generate before/after report
    # Return optimization results

def create_optimization_preview(file_path: str, config: Dict) -> Dict:
    """Show changes before applying"""
    # Run optimizations in dry-run mode
    # Generate diff (before â†’ after)
    # Highlight all changes
    # Provide approval prompt
    # Return preview report

def apply_selective_optimizations(file_path: str, selected: List[str]) -> Dict:
    """Apply only selected optimization types"""
    # User chooses: content, platform, voice, citations
    # Apply only selected modules
    # Preserve original for other aspects
    # Return partial optimization results

def batch_optimize_directory(directory: str, config: Dict) -> List[Dict]:
    """Optimize all files in directory"""
    # Find all HTML/MD files
    # Prioritize by current score (lowest first)
    # Apply optimizations with progress tracking
    # Generate batch report
    # Return results for all files

def configure_optimization_profile(name: str, settings: Dict) -> Dict:
    """Save reusable optimization profile"""
    # Platform targets (ChatGPT, Perplexity, etc.)
    # Aggressiveness level (conservative â†’ aggressive)
    # Update frequency preferences
    # Content style preferences
    # Save as JSON profile
    # Return configuration
```

**User Control**:
- Preview before applying (dry-run mode)
- Selective optimization (choose modules)
- Aggressiveness levels: Conservative | Balanced | Aggressive
- Undo capability (backup original)

**Optimization Profiles**:

```python
PROFILES = {
    "chatgpt_authority": {
        "platform": ["chatgpt"],
        "min_word_count": 1500,
        "add_credentials": True,
        "citation_style": "academic",
        "structure": "answer_first"
    },
    "perplexity_fresh": {
        "platform": ["perplexity"],
        "update_frequency": "weekly",
        "citation_style": "inline_numbers",
        "structure": "h2_h3_bullets",
        "emphasis": "freshness"
    },
    "multi_platform": {
        "platform": ["chatgpt", "perplexity", "claude"],
        "min_word_count": 1200,
        "update_frequency": "monthly",
        "citation_style": "mixed",
        "structure": "balanced"
    },
    "voice_optimized": {
        "platform": ["google_assistant", "siri"],
        "add_faq": True,
        "add_speakable": True,
        "featured_snippet": True,
        "reading_level": 8
    }
}
```

---

## Testing Strategy

### Test Cases

**1. Basic Content Optimization**:
- Input: `/tmp/test-page.html` (current basic page)
- Expected: TL;DR added, paragraphs split, FAQ section, evidence table
- Metrics: Word count 63â†’800+, score 43â†’70+

**2. Platform-Specific Optimization**:
- Input: Medical clinic page
- Target: ChatGPT (authority)
- Expected: Credentials prominent, 1500+ words, Article schema, citations
- Metrics: Score 70â†’85+

**3. Freshness Enhancement**:
- Input: Page with 2023 statistics
- Expected: Outdated elements identified, update recommendations
- Metrics: Freshness issues flagged, update schedule generated

**4. Voice Optimization**:
- Input: How-to content
- Expected: Featured snippet content, Speakable schema, HowTo schema
- Metrics: 30-40 word answers, 20-30 second segments

**5. Citation Enhancement**:
- Input: Content with claims but no statistics
- Expected: Citation opportunities identified, evidence table suggested
- Metrics: 5+ citation opportunities found

**6. Full Auto-Optimization**:
- Input: `/tmp/test-page.html`
- Config: Multi-platform profile
- Expected: All optimizations applied, before/after report
- Metrics: Score 43â†’80+, all recommendations addressed

### Quality Assurance

**Automated Checks**:
- No fact hallucination (all claims from original)
- No fabricated statistics or quotes
- Valid JSON-LD schema output
- HTML/Markdown syntax valid
- All links working
- Metadata complete

**Manual Review Required**:
- Content meaning preserved
- Tone and style appropriate
- Brand voice maintained
- Technical accuracy verified

---

## Integration with Phase 1

### Workflow Update

**Phase 1 (Analysis)**:
```
User: Audit this page
â†’ audit_report.py
â†’ Report: Score 43/100, issues found, recommendations
â†’ User reviews report
```

**Phase 2 (Implementation)**:
```
User: Optimize this page for ChatGPT
â†’ auto_implementer.py --platform chatgpt
  â†’ Runs audit_report.py (analysis)
  â†’ Runs content_optimizer.py (structure)
  â†’ Runs platform_optimizer.py (ChatGPT mode)
  â†’ Runs citation_enhancer.py (add evidence)
  â†’ Generates preview
â†’ User approves
â†’ Optimizations applied
â†’ Before/after report generated
```

### File Organization

```
seo-geo-optimizer/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ # Phase 1 (Analysis)
â”‚   â”œâ”€â”€ analyze_content.py
â”‚   â”œâ”€â”€ metadata_validator.py
â”‚   â”œâ”€â”€ keyword_analyzer.py
â”‚   â”œâ”€â”€ entity_extractor.py
â”‚   â”œâ”€â”€ schema_generator.py
â”‚   â”œâ”€â”€ audit_report.py
â”‚   â”‚
â”‚   â””â”€â”€ # Phase 2 (Implementation)
â”‚       â”œâ”€â”€ content_optimizer.py       # NEW
â”‚       â”œâ”€â”€ platform_optimizer.py      # NEW
â”‚       â”œâ”€â”€ voice_optimizer.py         # NEW
â”‚       â”œâ”€â”€ freshness_monitor.py       # NEW
â”‚       â”œâ”€â”€ citation_enhancer.py       # NEW
â”‚       â””â”€â”€ auto_implementer.py        # NEW
â”‚
â”œâ”€â”€ profiles/
â”‚   â”œâ”€â”€ chatgpt_authority.json         # NEW
â”‚   â”œâ”€â”€ perplexity_fresh.json          # NEW
â”‚   â”œâ”€â”€ multi_platform.json            # NEW
â”‚   â””â”€â”€ voice_optimized.json           # NEW
â”‚
â””â”€â”€ examples/
    â””â”€â”€ phase2/                         # NEW
        â”œâ”€â”€ before-optimization/
        â”‚   â””â”€â”€ test-page.html
        â”œâ”€â”€ after-optimization/
        â”‚   â”œâ”€â”€ test-page-chatgpt.html
        â”‚   â”œâ”€â”€ test-page-perplexity.html
        â”‚   â””â”€â”€ test-page-multi.html
        â””â”€â”€ optimization-report.md
```

---

## Success Metrics

### Quantitative Targets

**Content Structure**:
- Paragraph length: 60-100 words (90%+ compliance)
- Sentence length: 15-20 words (80%+ compliance)
- TL;DR: Present in first 60 words (100%)
- FAQ section: 3+ questions with 40-60 word answers (100%)

**Platform Optimization**:
- ChatGPT: 1500-2500 words, author credentials, Article schema (100%)
- Perplexity: H2â†’H3â†’bullets, inline citations, fresh dateModified (100%)
- Claude: 5-8 primary source citations, methodology section (100%)
- Voice: 30-40 word answers, Speakable schema, HowTo schema (100%)

**Citation Enhancement**:
- Statistics: 3+ statistics per 500 words
- Quotations: 1+ expert quote per major section
- Evidence tables: 1+ table for fact-heavy content
- References: Complete references section with clickable citations

**Score Improvements**:
- Low scores (0-40): +30-40 points â†’ 70-80 range
- Medium scores (41-70): +20-30 points â†’ 80-90 range
- High scores (71-90): +5-10 points â†’ 85-95 range

### Qualitative Targets

**Content Quality**:
- Meaning preserved (100% accuracy)
- Tone and style maintained
- Brand voice consistent
- No hallucinated facts

**User Experience**:
- Preview before applying (approval workflow)
- Clear diff view (before â†’ after)
- Undo capability (backup original)
- Selective optimization (choose modules)

**Performance**:
- Content optimization: <5 seconds
- Full auto-optimization: <15 seconds
- Batch optimization: <3 seconds per file
- No external dependencies (Python stdlib only)

---

## Risk Management

### Potential Issues

**1. Content Meaning Changed**:
- Risk: Optimization alters original meaning
- Mitigation: Preserve all facts, manual review gate, undo capability
- Fallback: Conservative mode (minimal changes)

**2. Over-Optimization**:
- Risk: Content becomes formulaic, loses personality
- Mitigation: Aggressiveness levels, selective optimization, style preservation
- Fallback: Manual editing after auto-optimization

**3. Fabricated Data**:
- Risk: System generates fake statistics or quotes
- Mitigation: NO data generation, only suggest where to add, user provides data
- Validation: Fact-checking gate before applying

**4. Schema Errors**:
- Risk: Invalid JSON-LD breaks rich snippets
- Mitigation: Schema validation, test all outputs, error handling
- Fallback: Revert to original schema if validation fails

### Quality Gates

**Gate 1: Analysis**:
- Audit existing content
- Identify optimization opportunities
- Generate recommendations
- â†’ Proceed to Gate 2

**Gate 2: Preview**:
- Apply optimizations (dry-run)
- Generate before/after diff
- Show all changes to user
- â†’ User approval required â†’ Gate 3

**Gate 3: Validation**:
- Validate HTML/Markdown syntax
- Validate JSON-LD schemas
- Check fact preservation
- Run metadata_validator.py
- â†’ All checks pass â†’ Gate 4

**Gate 4: Implementation**:
- Backup original file
- Apply optimizations
- Generate optimization report
- â†’ Complete

**Gate 5: Verification** (optional):
- Re-run audit on optimized file
- Compare scores (before â†’ after)
- Verify all recommendations addressed
- â†’ Success

---

## Timeline

### Week 1-2: Content Rewriting
- Build `content_optimizer.py`
- Test paragraph/sentence optimization
- Test TL;DR generation
- Test FAQ section creation
- Test evidence table formatting
- Test heading structure optimization

### Week 3-4: Platform Optimization
- Build `platform_optimizer.py`
- Implement ChatGPT optimization
- Implement Perplexity optimization
- Implement Claude optimization
- Implement Gemini optimization
- Test multi-platform mode

### Week 5-6: Voice & Freshness
- Build `voice_optimizer.py`
- Build `freshness_monitor.py`
- Test featured snippet generation
- Test Speakable schema
- Test freshness analysis
- Test update recommendations

### Week 6-7: Citation & Orchestration
- Build `citation_enhancer.py`
- Build `auto_implementer.py`
- Create optimization profiles
- Build preview/approval workflow
- Test full pipeline

### Week 8: Testing & Documentation
- Complete test suite
- Update SKILL.md with Phase 2
- Create Phase 2 examples
- Update README.md
- Commit and push to GitHub

---

## Next Steps

1. **Review this plan** - Confirm approach and priorities
2. **Begin implementation** - Start with `content_optimizer.py`
3. **Iterative testing** - Test each script as built
4. **User feedback** - Validate optimizations maintain quality
5. **Documentation** - Update SKILL.md with new capabilities

---

**Status**: âœ… Plan complete, ready for implementation
**Research Sources**: 41M AI search results, 680M citations (2025 data)
**Key Insight**: Statistics Addition (+41%), Quotation Addition (+28%), Freshness (3.2x citations)
**Target**: Transform analysis-only tool into full optimization pipeline
